{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Correctness of GPTQ-triton\n",
    "\n",
    "This notebook verifies the correctness of the Triton kernels and other modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import itertools\n",
    "\n",
    "import original_quant\n",
    "import gptq_triton\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaMLP, LlamaConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify QuantLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   M   |   N   |   K   | cuda - ref | triton - ref | triton - cuda |\n",
      "     1 |  4096 |  4096 |   0.000977 |     0.001953 |      0.001953 | \n",
      "     1 |  4096 | 11008 |   0.000977 |     0.001953 |      0.001953 | \n",
      "     1 | 11008 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "     1 | 11008 | 11008 |   0.000977 |     0.001953 |      0.000977 | \n",
      "     8 |  4096 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "     8 |  4096 | 11008 |   0.001953 |     0.001953 |      0.001953 | \n",
      "     8 | 11008 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "     8 | 11008 | 11008 |   0.001953 |     0.001953 |      0.001953 | \n",
      "   100 |  4096 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "   100 |  4096 | 11008 |   0.001953 |     0.001953 |      0.001953 | \n",
      "   100 | 11008 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "   100 | 11008 | 11008 |   0.001953 |     0.002930 |      0.001953 | \n",
      "   256 |  4096 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "   256 |  4096 | 11008 |   0.001953 |     0.002930 |      0.002930 | \n",
      "   256 | 11008 |  4096 |   0.001953 |     0.003906 |      0.001953 | \n",
      "   256 | 11008 | 11008 |   0.001953 |     0.003906 |      0.002930 | \n",
      "  2048 |  4096 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "  2048 |  4096 | 11008 |   0.001953 |     0.002930 |      0.002930 | \n",
      "  2048 | 11008 |  4096 |   0.001953 |     0.001953 |      0.001953 | \n",
      "  2048 | 11008 | 11008 |   0.001953 |     0.003906 |      0.001953 | \n"
     ]
    }
   ],
   "source": [
    "# QuantLinear is compared against a reference and the CUDA kernel at various values of M, N and K\n",
    "# The reference is an FP16 simulation of the quantized weights\n",
    "torch.manual_seed(0)\n",
    "print(\"   M   |   N   |   K   | cuda - ref | triton - ref | triton - cuda |\")\n",
    "\n",
    "for (M, N, K) in itertools.product([1, 8, 100, 256, 2048], [4096, 11008], [4096, 11008]):\n",
    "\tM = M # B * seq_len\n",
    "\tK = K # Input dimension\n",
    "\tN = N # Output dimension\n",
    "\n",
    "\tlayer = nn.Linear(K, N, bias=False)  # Llama doesn't use bias\n",
    "\tvec = torch.randn(1, M, K, device='cuda', dtype=torch.float16)\n",
    "\n",
    "\tquantizer = original_quant.Quantizer()\n",
    "\tquantizer.configure(4, perchannel=True, sym=False, mse=False)\n",
    "\tquantizer.find_params(layer.weight.data, weight=True)\n",
    "\tlayer.weight.data = original_quant.quantize(layer.weight.data, quantizer.scale, quantizer.zero, quantizer.maxq)\n",
    "\n",
    "\tcudalayer = original_quant.QuantLinear(4, -1, layer.in_features, layer.out_features)\n",
    "\tcudalayer.pack(layer, quantizer.scale, quantizer.zero)\n",
    "\n",
    "\ttritonlayer = gptq_triton.QuantLinear(4, -1, layer.in_features, layer.out_features, bias=False)\n",
    "\tstate_dict = cudalayer.state_dict()\n",
    "\tstate_dict['scales'] = state_dict['scales'].to(torch.float16, copy=True)\n",
    "\tdel state_dict['bias']\n",
    "\ttritonlayer.load_state_dict(state_dict)\n",
    "\n",
    "\tlayer = layer.half()\n",
    "\n",
    "\tlayer = layer.to('cuda')\n",
    "\tcudalayer = cudalayer.to('cuda')\n",
    "\ttritonlayer = tritonlayer.to('cuda')\n",
    "\n",
    "\tref = layer(vec)\n",
    "\tcuda_out = cudalayer(vec)\n",
    "\ttriton_out = tritonlayer(vec)\n",
    "\n",
    "\t# Print results\n",
    "\tprint(f' {M:5d}', end=' | ')\n",
    "\tprint(f'{N:5d}', end=' | ')\n",
    "\tprint(f'{K:5d}', end=' | ')\n",
    "\tprint(f'  {(cuda_out - ref).abs().max():.6f}', end=' | ')\n",
    "\tprint(f'    {(triton_out - ref).abs().max():.6f}', end=' | ')\n",
    "\tprint(f'     {(triton_out - cuda_out).abs().max():.6f}', end=' | ')\n",
    "\n",
    "\tif (triton_out - ref).abs().max() > 0.004 or (triton_out - cuda_out).abs().max() > 0.004:\n",
    "\t\tprint(\" !!! WARNING: Error is too large !!! \")\n",
    "\telse:\n",
    "\t\tprint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify QKV Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 0.0\n",
      "Max diff: 0.0\n",
      "Max diff: 0.0\n",
      "Max diff: 0.0\n",
      "Max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Comparison to ensure that the QKV fusion is correct\n",
    "class TestModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attn = LlamaAttention(LlamaConfig(hidden_size=4096, num_attention_heads=32))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.attn(x,)\n",
    "\n",
    "model = TestModel()\n",
    "\n",
    "# Quantize the model\n",
    "for name, m in model.named_modules():\n",
    "\tif not isinstance(m, nn.Linear):\n",
    "\t\tcontinue\n",
    "\n",
    "\tquantizer = original_quant.Quantizer()\n",
    "\tquantizer.configure(4, perchannel=True, sym=False, mse=False)\n",
    "\tquantizer.find_params(m.weight.data, weight=True)\n",
    "\tm.weight.data = original_quant.quantize(m.weight.data, quantizer.scale, quantizer.zero, quantizer.maxq).to(torch.float16)\n",
    "\n",
    "\tqlayer = original_quant.QuantLinear(4, -1, m.in_features, m.out_features)\n",
    "\tqlayer.pack(m, quantizer.scale, quantizer.zero)\n",
    "\tstate_dict = qlayer.state_dict()\n",
    "\n",
    "\ttritonlayer = gptq_triton.QuantLinear(4, -1, m.in_features, m.out_features, bias=False)\n",
    "\tstate_dict = qlayer.state_dict()\n",
    "\tstate_dict['scales'] = state_dict['scales'].to(torch.float16, copy=True)\n",
    "\tdel state_dict['bias']\n",
    "\ttritonlayer.load_state_dict(state_dict)\n",
    "\n",
    "\t# Replace in model\n",
    "\tparent_name = name.rsplit('.', 1)[0]\n",
    "\tparent = model.get_submodule(parent_name)\n",
    "\n",
    "\tsetattr(parent, name[len(parent_name) + 1:], tritonlayer)\n",
    "\n",
    "# Save the original attention layer\n",
    "original_attn = model.attn\n",
    "\n",
    "# Fuse\n",
    "gptq_triton.make_quant_attn(model)\n",
    "fused_attn = model.attn\n",
    "\n",
    "# Move to CUDA\n",
    "original_attn.to('cuda')\n",
    "fused_attn.to('cuda')\n",
    "\n",
    "# Compare\n",
    "for M in [1, 8, 100, 256, 2048]:\n",
    "\tx = torch.randn(1, M, 4096, device='cuda', dtype=torch.float16)\n",
    "\tposition_ids = torch.arange(0, M, dtype=torch.long, device='cuda')\n",
    "\tposition_ids = position_ids.unsqueeze(0).view(-1, M)\n",
    "\n",
    "\toriginal_out = original_attn(x, position_ids=position_ids)[0]\n",
    "\tfused_out = fused_attn(x, position_ids=position_ids)[0]\n",
    "\n",
    "\tdiff = (original_out - fused_out).abs().max()\n",
    "\tprint(f\"Max diff: {diff}\")\n",
    "\n",
    "\t# Assertions\n",
    "\tassert isinstance(fused_attn, gptq_triton.QuantLlamaAttention)\n",
    "\tassert diff == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Fused MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 0.000244140625\n",
      "Max diff: 0.000244140625\n",
      "Max diff: 0.000244140625\n",
      "Max diff: 0.0002593994140625\n",
      "Max diff: 0.00048828125\n"
     ]
    }
   ],
   "source": [
    "layer = LlamaMLP(4096, 11008, 'silu')\n",
    "layer = layer.half()\n",
    "\n",
    "# Quantize\n",
    "for name, m in layer.named_modules():\n",
    "\tif not isinstance(m, nn.Linear):\n",
    "\t\tcontinue\n",
    "\n",
    "\tquantizer = original_quant.Quantizer()\n",
    "\tquantizer.configure(4, perchannel=True, sym=False, mse=False)\n",
    "\tquantizer.find_params(m.weight.data, weight=True)\n",
    "\tm.weight.data = original_quant.quantize(m.weight.data, quantizer.scale, quantizer.zero, quantizer.maxq).to(torch.float16)\n",
    "\n",
    "\tqlayer = original_quant.QuantLinear(4, -1, m.in_features, m.out_features)\n",
    "\tqlayer.pack(m, quantizer.scale, quantizer.zero)\n",
    "\tstate_dict = qlayer.state_dict()\n",
    "\n",
    "\ttritonlayer = gptq_triton.QuantLinear(4, -1, m.in_features, m.out_features, bias=False)\n",
    "\tstate_dict['scales'] = state_dict['scales'].to(torch.float16, copy=True)\n",
    "\tdel state_dict['bias']\n",
    "\ttritonlayer.load_state_dict(state_dict)\n",
    "\n",
    "\tsetattr(layer, name, tritonlayer)\n",
    "\n",
    "# Fuse\n",
    "fused_layer = gptq_triton.make_fused_mlp(layer)\n",
    "assert isinstance(fused_layer, gptq_triton.QuantLlamaMLP)\n",
    "\n",
    "# Move to CUDA\n",
    "layer.to('cuda')\n",
    "fused_layer.to('cuda')\n",
    "\n",
    "# Compare\n",
    "for M in [1, 8, 100, 256, 2048]:\n",
    "\tx = torch.randn(1, M, 4096, device='cuda', dtype=torch.float16)\n",
    "\n",
    "\toriginal_out = layer(x)\n",
    "\tfused_out = fused_layer(x)\n",
    "\n",
    "\tdiff = (original_out - fused_out).abs().max()\n",
    "\tprint(f\"Max diff: {diff}\")\n",
    "\n",
    "\t# There is a small difference because the fused MLP performs some calculations in float32, while the original MLP performs them in float16\n",
    "\tassert diff < 1e-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
